{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e96f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a29b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('balanced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19086fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = data[data['category'] == 'Shirts/Tops']\n",
    "reviews = tops['text'].tolist()\n",
    "ratings = tops['rating'].astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd689618",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [str(i) for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c403bb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'rating', 'parent_asin', 'text', 'title',\n",
       "       'average_rating', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dc1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "#topics, probs = topic_model.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ed8b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd3db01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933ea2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2)   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc130ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44f8d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2516c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "tops['topic'] = topics\n",
    "tops['topic_probs'] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488680ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fee77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "543e0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/praneethapratapa/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/praneethapratapa/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment scores calculated. Range: -1.000 to 1.000\n",
      "Mean sentiment: 0.158\n"
     ]
    }
   ],
   "source": [
    "tops_cleaned = tops[tops['topic'] != -1]\n",
    "\n",
    "# Calculate sentiment polarity for each review\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data (only needed first time)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Polarity ranges from -1 (negative) to 1 (positive)\n",
    "sentiment_scores = []\n",
    "for review in tops_cleaned['text']:\n",
    "    blob = TextBlob(str(review))\n",
    "    sentiment_scores.append(blob.sentiment.polarity)\n",
    "\n",
    "tops_cleaned['sentiment_polarity'] = sentiment_scores\n",
    "\n",
    "print(f\"Sentiment scores calculated. Range: {min(sentiment_scores):.3f} to {max(sentiment_scores):.3f}\")\n",
    "print(f\"Mean sentiment: {np.mean(sentiment_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9fdb758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>category</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_probs</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B08NHJ5S5H</td>\n",
       "      <td>Not long enough. I like shirt that cover or be...</td>\n",
       "      <td>Dokotoo Women's Ladies Spring Basic Ribbed Str...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Shirts/Tops</td>\n",
       "      <td>12</td>\n",
       "      <td>0.757932</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>B071F91SCM</td>\n",
       "      <td>Very pretty, but the post is super thick and t...</td>\n",
       "      <td>925 Sterling Silver Cubic Zirconia Purple Butt...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Shirts/Tops</td>\n",
       "      <td>17</td>\n",
       "      <td>0.237646</td>\n",
       "      <td>0.119444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B08JGGTCB9</td>\n",
       "      <td>I got it for my bf for his birthday was disapp...</td>\n",
       "      <td>Jordan Paris Saint-Germain Long-Sleeve T-Shirt...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Shirts/Tops</td>\n",
       "      <td>231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>B01IP4GEA2</td>\n",
       "      <td>Small and itchy.</td>\n",
       "      <td>Romwe Women's Striped Crewneck Short Sleeve Lo...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Shirts/Tops</td>\n",
       "      <td>158</td>\n",
       "      <td>0.908018</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B01EN4KVRK</td>\n",
       "      <td>Cute little dress but does run on the smaller ...</td>\n",
       "      <td>Tenworld Women Crew Neck Short Sleeve Striped ...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Shirts/Tops</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  rating parent_asin  \\\n",
       "0            0     2.0  B08NHJ5S5H   \n",
       "3            3     3.0  B071F91SCM   \n",
       "9            9     4.0  B08JGGTCB9   \n",
       "17          17     3.0  B01IP4GEA2   \n",
       "61          61     4.0  B01EN4KVRK   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Not long enough. I like shirt that cover or be...   \n",
       "3   Very pretty, but the post is super thick and t...   \n",
       "9   I got it for my bf for his birthday was disapp...   \n",
       "17                                   Small and itchy.   \n",
       "61  Cute little dress but does run on the smaller ...   \n",
       "\n",
       "                                                title  average_rating  \\\n",
       "0   Dokotoo Women's Ladies Spring Basic Ribbed Str...             3.6   \n",
       "3   925 Sterling Silver Cubic Zirconia Purple Butt...             4.7   \n",
       "9   Jordan Paris Saint-Germain Long-Sleeve T-Shirt...             4.0   \n",
       "17  Romwe Women's Striped Crewneck Short Sleeve Lo...             3.3   \n",
       "61  Tenworld Women Crew Neck Short Sleeve Striped ...             3.4   \n",
       "\n",
       "       category  topic  topic_probs  sentiment_polarity  \n",
       "0   Shirts/Tops     12     0.757932            0.012500  \n",
       "3   Shirts/Tops     17     0.237646            0.119444  \n",
       "9   Shirts/Tops    231     1.000000           -0.125000  \n",
       "17  Shirts/Tops    158     0.908018           -0.250000  \n",
       "61  Shirts/Tops      0     1.000000            0.177083  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe9d6d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Got xl fits like a medium, Ordered a large bu...\n",
      "Name: Representative_Docs, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#topic_model.get_topic_info(1)['Representative_Docs']\n",
    "\n",
    "print(topic_model.get_topic_info(2)['Representative_Docs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3487858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The shirt arrived within 48 hours with prime, however I was so disappointed. The material is rayon/polyester versus cotton and looks like it will melt if you attempt to iron it. Also, the holes have no rhyme or reason and look like someone ways rushing to get the shirt off the press and to the customer so they cut random gignormous holes in different places. There is a gaping hole in the abs section and other unforgiving holes throughout the shirt but not resembling the advertised photo. This a very amateur attempt at making a distressed top as I could've made a better one on my own. I am returning it.\",\n",
       " 'Product is great.  I rarely have anything negative to comment about regarding Amazon but this time I feel I need to offer my opinion - packaging and delivery are top notch 95% of the time.  This delivery came without any outside packaging - in other words, it was delivered in a clear plastic bag for all to see from my front steps.  Granted, the delivery was only a set of big boys PJs but it would have counted if it was taken and I had to go out to a department store to replace it.  Thank you.',\n",
       " \"Always an adventure ordering the mainland China/Korea menswear. Four times out of five, I like what I get. It's just not what I thought I was getting. This time, I thought I was getting a fairly fitted, thick sweater. What I got was an extremely loose & floppy tunic made out of (apparently) cooked pasta. Not even al dente cooked pasta. I know this is going to go south (literally) after I wash it a few times, but in the meantime it looks good. If I were that kind of gal, I'd wear it with leggings & boots. But I'm not, but you could. Like most orders from this part of the world, it got to me in Pacific Northwest U.S. of A. in a little less than 2 weeks. Oh & my item is gray & darker gray, like I expected. None of the brownish/yellowish/bluish shades other reviewers mentioned.\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_representative_docs(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64def264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>45_delivery_shipping_fast_arrived</td>\n",
       "      <td>[delivery, shipping, fast, arrived, shipped, q...</td>\n",
       "      <td>[The shirt arrived within 48 hours with prime,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                               Name  \\\n",
       "0     45     83  45_delivery_shipping_fast_arrived   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [delivery, shipping, fast, arrived, shipped, q...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [The shirt arrived within 48 hours with prime,...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b7a84b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>15494</td>\n",
       "      <td>-1_shirt_size_material_like</td>\n",
       "      <td>[shirt, size, material, like, fit, small, fabr...</td>\n",
       "      <td>[It's good, Fabric is almost see through, Not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1740</td>\n",
       "      <td>0_dress_love dress_dresses_cute dress</td>\n",
       "      <td>[dress, love dress, dresses, cute dress, pocke...</td>\n",
       "      <td>[LOVE this dress!!!, Love dress, Dress is see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>959</td>\n",
       "      <td>1_tops_cute_love_beautiful</td>\n",
       "      <td>[tops, cute, love, beautiful, fits, flattering...</td>\n",
       "      <td>[I actually really like this product, but it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>468</td>\n",
       "      <td>2_xl_xxl_medium_large</td>\n",
       "      <td>[xl, xxl, medium, large, fits like, ordered, o...</td>\n",
       "      <td>[Got xl fits like a medium, Ordered a large bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>357</td>\n",
       "      <td>3_blouse_nice blouse_blouses_cute blouse</td>\n",
       "      <td>[blouse, nice blouse, blouses, cute blouse, be...</td>\n",
       "      <td>[Really like this blouse., cute blouse., Very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>381</td>\n",
       "      <td>10</td>\n",
       "      <td>381_soft material_length sized_shorter stated_...</td>\n",
       "      <td>[soft material, length sized, shorter stated, ...</td>\n",
       "      <td>[Soft and stretchy but not as supportive as I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>382</td>\n",
       "      <td>10</td>\n",
       "      <td>382_shirt awesome_awesome shirt_shirt cute_way...</td>\n",
       "      <td>[shirt awesome, awesome shirt, shirt cute, way...</td>\n",
       "      <td>[Awesome shirt, AWESOME SHIRT, Awesome shirt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>383</td>\n",
       "      <td>10</td>\n",
       "      <td>383_bigger br_just felt_felt like_little larger</td>\n",
       "      <td>[bigger br, just felt, felt like, little large...</td>\n",
       "      <td>[I loved it on the Today show and they said it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>384</td>\n",
       "      <td>10</td>\n",
       "      <td>384_lbs 5ft_34 110_115 lbs_110</td>\n",
       "      <td>[lbs 5ft, 34 110, 115 lbs, 110, 115, 5ft, look...</td>\n",
       "      <td>[I bought this and it's cute but once I got it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>385</td>\n",
       "      <td>10</td>\n",
       "      <td>385_150lbs_zips kinda_dressy flats_isn sown</td>\n",
       "      <td>[150lbs, zips kinda, dressy flats, isn sown, l...</td>\n",
       "      <td>[Very sexy love the way that it looks..I got a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0       -1  15494                        -1_shirt_size_material_like   \n",
       "1        0   1740              0_dress_love dress_dresses_cute dress   \n",
       "2        1    959                         1_tops_cute_love_beautiful   \n",
       "3        2    468                              2_xl_xxl_medium_large   \n",
       "4        3    357           3_blouse_nice blouse_blouses_cute blouse   \n",
       "..     ...    ...                                                ...   \n",
       "382    381     10  381_soft material_length sized_shorter stated_...   \n",
       "383    382     10  382_shirt awesome_awesome shirt_shirt cute_way...   \n",
       "384    383     10    383_bigger br_just felt_felt like_little larger   \n",
       "385    384     10                     384_lbs 5ft_34 110_115 lbs_110   \n",
       "386    385     10        385_150lbs_zips kinda_dressy flats_isn sown   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [shirt, size, material, like, fit, small, fabr...   \n",
       "1    [dress, love dress, dresses, cute dress, pocke...   \n",
       "2    [tops, cute, love, beautiful, fits, flattering...   \n",
       "3    [xl, xxl, medium, large, fits like, ordered, o...   \n",
       "4    [blouse, nice blouse, blouses, cute blouse, be...   \n",
       "..                                                 ...   \n",
       "382  [soft material, length sized, shorter stated, ...   \n",
       "383  [shirt awesome, awesome shirt, shirt cute, way...   \n",
       "384  [bigger br, just felt, felt like, little large...   \n",
       "385  [lbs 5ft, 34 110, 115 lbs, 110, 115, 5ft, look...   \n",
       "386  [150lbs, zips kinda, dressy flats, isn sown, l...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [It's good, Fabric is almost see through, Not ...  \n",
       "1    [LOVE this dress!!!, Love dress, Dress is see ...  \n",
       "2    [I actually really like this product, but it's...  \n",
       "3    [Got xl fits like a medium, Ordered a large bu...  \n",
       "4    [Really like this blouse., cute blouse., Very ...  \n",
       "..                                                 ...  \n",
       "382  [Soft and stretchy but not as supportive as I'...  \n",
       "383      [Awesome shirt, AWESOME SHIRT, Awesome shirt]  \n",
       "384  [I loved it on the Today show and they said it...  \n",
       "385  [I bought this and it's cute but once I got it...  \n",
       "386  [Very sexy love the way that it looks..I got a...  \n",
       "\n",
       "[387 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e76dd",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72273bc4",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "046fa46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (18439, 2)\n",
      "  - Topic probabilities: 1 features\n",
      "  - Sentiment polarity: 1 feature\n",
      "MSE: 1.3886376253463302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert topic_probs Series to a 2D numpy array\n",
    "topic_features = np.vstack(tops_cleaned['topic_probs'].values)\n",
    "\n",
    "# Add sentiment polarity as an additional feature\n",
    "sentiment_features = tops_cleaned['sentiment_polarity'].values.reshape(-1, 1)\n",
    "\n",
    "# Combine topic probabilities with sentiment features\n",
    "combined_features = np.hstack([topic_features, sentiment_features])\n",
    "\n",
    "print(f\"Feature shape: {combined_features.shape}\")\n",
    "print(f\"  - Topic probabilities: {topic_features.shape[1]} features\")\n",
    "print(f\"  - Sentiment polarity: 1 feature\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_features, tops_cleaned['rating'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "\n",
    "print(\"MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef1eb441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1784046950629186\n"
     ]
    }
   ],
   "source": [
    "#calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print (\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9818f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_model = r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29d72fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2850046990905425\n"
     ]
    }
   ],
   "source": [
    "print (r2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217f717",
   "metadata": {},
   "source": [
    "###### We saw that even after removing stopwords, only 0.025% of the variability in our outcome variable is explained by our model (using default BERTopic embedding model all-MiniLM-L6-v2). Learned that sentence‑transformer models (e.g., mpnet, MiniLM, e5) are trained on natural sentences that contain stop words. the R2 improved to 0.165% after using \"all-mpnet-base-v2\" model. Adding sentiment improved the R2 to 0.285 for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24958952",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c15a729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Performance:\n",
      "MSE: 1.3557\n",
      "RMSE: 1.1643\n",
      "R² Score: 0.3020\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "xgb_mse = mean_squared_error(y_test, xgb_preds)\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "xgb_r2 = r2_score(y_test, xgb_preds)\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(f\"MSE: {xgb_mse:.4f}\")\n",
    "print(f\"RMSE: {xgb_rmse:.4f}\")\n",
    "print(f\"R² Score: {xgb_r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
